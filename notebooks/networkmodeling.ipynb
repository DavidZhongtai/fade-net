{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Convolution1D\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "#import statments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2a98f2e97f20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtoken_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentify_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mstops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7546\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7547\u001b[0m         )\n\u001b[0;32m-> 7548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7550\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                         \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2a98f2e97f20>\u001b[0m in \u001b[0;36midentify_tokens\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0midentify_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bodytext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# taken only words (not punctuation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtoken_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \"\"\"\n\u001b[0;32m-> 1272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \"\"\"\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \"\"\"\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \"\"\"\n\u001b[1;32m   1356\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after_tok\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/davidan/Downloads/fade-net/data/data_shuffled.csv')\n",
    "data['bodytext'] = data['bodytext'].str.lower()\n",
    "\n",
    "def identify_tokens(row):\n",
    "    news = row['bodytext']\n",
    "    tokens = nltk.word_tokenize(news)\n",
    "    # taken only words (not punctuation)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data['words'] = data.apply(identify_tokens, axis=1)\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['words']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data['stem_meaningful'] = data.apply(remove_stops, axis=1)\n",
    "\n",
    "def rejoin_words(row):\n",
    "    my_list = row['stem_meaningful']\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    return joined_words\n",
    "\n",
    "data['processed'] = data.apply(rejoin_words, axis=1)\n",
    "\n",
    "# cols_to_drop = ['words', 'stem_meaningful']\n",
    "# data.drop(cols_to_drop, inplace=True)\n",
    "\n",
    "data['bodytext'] = data['processed']\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcol = ['words', 'stem_meaningful', 'processed']\n",
    "data.drop(labels = dropcol, axis = 1)\n",
    "\n",
    "data.to_csv('/Users/davidan/Downloads/fade-net/data/tokenizedData.csv', index = False)\n",
    "\n",
    "#manually check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/davidan/Downloads/fade-net/data/tokenizedData.csv')\n",
    "data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 1. 1.]\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/davidan/Downloads/fade-net/data/tokenizedData.csv')\n",
    "\n",
    "\n",
    "news = data['bodytext'].values\n",
    "label = data['label'].values\n",
    "\n",
    "#delete the header file \n",
    "news = np.delete(news, (4396), axis=0)\n",
    "label = np.delete(label, (4396), axis=0)\n",
    "\n",
    "counter_news = 0\n",
    "\n",
    "for entry in news:\n",
    "    news[counter_news] = news[counter_news][:244]\n",
    "\n",
    "label = np.array(label)\n",
    "\n",
    "label = label.astype(np.float)\n",
    "\n",
    "\n",
    "\n",
    "print(label)\n",
    "print(type(label[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guardian survey found pervasive culture staff bullying parts detrimental impact staff wellbeing patient stories professionals submitted survey guardian diagnosed acute stress incident critically high blood frightened one man thought nothing sex\n"
     ]
    }
   ],
   "source": [
    "print(news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News: guardian survey found pervasive culture staff bullying parts detrimental impact staff wellbeing patient stories professionals submitted survey guardian diagnosed acute stress incident critically high blood frightened one man thought nothing sex\n",
      "Representation: \n",
      "[413, 2105, 92, 8134, 648, 444, 5276, 991, 9492, 532, 444, 5702, 2753, 600, 3014, 4103, 2105, 413, 4981, 5787, 2292, 1315, 7115, 146, 1053, 6897, 2, 97, 228, 188, 911]\n",
      "Average Length 244\n",
      "vocabulary size: 48543\n",
      "Max Length: 2524\n",
      "Min Length: 1\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(news)\n",
    "\n",
    "text_matrix = t.texts_to_sequences(news)\n",
    "\n",
    "print('News: ' + news[0])\n",
    "print('Representation: ')\n",
    "print(text_matrix[0])\n",
    "\n",
    "#find max length of vectors\n",
    "\n",
    "max_length = 0\n",
    "min_length = 1\n",
    "total_length = 0\n",
    "\n",
    "emb_dim = 16\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "for i in range(len(text_matrix)):\n",
    "    sent_length = len(text_matrix[i])\n",
    "    if max_length < sent_length:\n",
    "        max_length = sent_length\n",
    "        \n",
    "for i in range(len(text_matrix)):\n",
    "    sent_length = len(text_matrix[i])\n",
    "    if min_length > sent_length:\n",
    "        min_length = sent_length\n",
    "\n",
    "for i in range(len(text_matrix)):\n",
    "    sent_length = len(text_matrix[i])\n",
    "    total_length += sent_length\n",
    "\n",
    "print('Average Length %d' % int(total_length/19255))\n",
    "print('vocabulary size: %d'%vocab_size)\n",
    "print('Max Length: %d' % max_length)\n",
    "print('Min Length: %d' % min_length)\n",
    "\n",
    "print(type(text_matrix[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[413, 2105, 92, 8134, 648, 444, 5276, 991, 9492, 532, 444, 5702, 2753, 600, 3014, 4103, 2105, 413, 4981, 5787, 2292, 1315, 7115, 146, 1053, 6897, 2, 97, 228, 188, 911]\n"
     ]
    }
   ],
   "source": [
    "print(text_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split datasets \n",
    "#No need to post pad data since all have lengths \n",
    "text_matrix = pad_sequences(text_matrix, maxlen=max_length, padding='post')\n",
    "\n",
    "# text_matrix = np.array(text_matrix, dtype=None)\n",
    "# label = np.array(label, dtype= None)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_matrix, label, test_size = 0.33, random_state = 53)\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "X_test = tf.convert_to_tensor(X_test)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_test = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 64, 16)            776688    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 12)                1392      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 778,093\n",
      "Trainable params: 778,093\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "lstm_units = 12\n",
    "inputs = Input(shape = (64,))\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=244, embeddings_regularizer=l2(.001))\n",
    "embd_out = embedding(inputs)\n",
    "\n",
    "lstm = LSTM(lstm_units, dropout = 0.25, recurrent_dropout = 0.2)\n",
    "lstm_out = lstm(embd_out)\n",
    "\n",
    "prob = Dense(1, activation = 'relu')\n",
    "outputs = prob(lstm_out)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "164/164 [==============================] - 508s 3s/step - loss: 7.6728 - acc: 0.5058 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 2/10\n",
      "164/164 [==============================] - 440s 3s/step - loss: 7.7659 - acc: 0.4965 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 3/10\n",
      "164/164 [==============================] - 402s 2s/step - loss: 7.6853 - acc: 0.5018 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 4/10\n",
      "164/164 [==============================] - 405s 2s/step - loss: 7.7203 - acc: 0.4995 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 5/10\n",
      "164/164 [==============================] - 415s 3s/step - loss: 7.7882 - acc: 0.4951 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 6/10\n",
      "164/164 [==============================] - 402s 2s/step - loss: 7.6755 - acc: 0.5024 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 7/10\n",
      "164/164 [==============================] - 375s 2s/step - loss: 7.7031 - acc: 0.5006 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 8/10\n",
      "164/164 [==============================] - 370s 2s/step - loss: 7.7215 - acc: 0.4994 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 9/10\n",
      "164/164 [==============================] - 397s 2s/step - loss: 7.6644 - acc: 0.5031 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 10/10\n",
      "164/164 [==============================] - 394s 2s/step - loss: 7.6851 - acc: 0.5018 - val_loss: 7.6429 - val_acc: 0.5045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9e86fc7ee0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "model.fit(x=X_train,y=y_train,\n",
    "          batch_size=80,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test,y_test)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMech(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionMech, self).__init__(**kwargs)\n",
    "    \n",
    "    # This method states the weights that the layer will learn. It has as input param the shape of the input\n",
    "    # which is called. This method is called at the declaration time\n",
    "    def build(self, input_shape):\n",
    "        # We need to provide the dimensions of our weights. In this example, we will have a W_a matrix of\n",
    "        # dimension (lstm_units, 1), and a bias of dimension (max_length, 1)\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(AttentionMech, self).build(input_shape)\n",
    "    \n",
    "    # In this method with do all the calculations of the layer and return the output of the layer\n",
    "    def call(self, x):\n",
    "        # x is the input of the layer. In this example, the output of lstm (hidden_statesxlstm_units) \n",
    "        # hidden_states = max_length\n",
    "        \n",
    "        # We calculate the score tanh(W.x + b)\n",
    "        scores = K.tanh(K.dot(x,self.W)+self.b)  # (max_length x 1) \n",
    "        print('scores shape: ')\n",
    "        print(scores.shape)\n",
    "        \n",
    "        # This removes the last axis -> a vector of max_length dimension \n",
    "        # we can omit this since our W matrix has dimension 1 in the last axis\n",
    "        scores=K.squeeze(scores, axis=-1) \n",
    "        print('scores shape after squeeze: ')\n",
    "        print(scores.shape)\n",
    "        \n",
    "        # we apply softmax (the last axis is the default axis used for calculation)\n",
    "        at=K.softmax(scores)\n",
    "        print('attention weights shape: ')\n",
    "        print(at.shape)\n",
    "        \n",
    "        # This adds a 1-sized dimension to the last axis -> matrix of (max_length x 1)\n",
    "        at=K.expand_dims(at,axis=-1) # if there is no squeeze, then we can omit this\n",
    "        print('attention weights shape after expand_dims: ')\n",
    "        print(at.shape)\n",
    "        \n",
    "        # We calculate the weighted values -> \\alpha*hidden_states         \n",
    "        # row-wise multiplication (we are weighting the hidden_states, not the lstm_units) \n",
    "        output=x*at # (max_length x lstm_units)\n",
    "        print('weighted values shape: ')\n",
    "        print(output)\n",
    "        \n",
    "        # The output of this layer is the weighted values (we sum up the values of the hidden states), and\n",
    "        # the weights of the attetnion (max_length x 1)\n",
    "        return K.sum(output, axis=1), at\n",
    "    \n",
    "    # This is used for summary, to see the output shape of the two output matrices\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "    \n",
    "    # This is used for summary (it returns the params of the layer)\n",
    "    def get_config(self):\n",
    "        return super(BahdanauAttention, self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"attention_mech/mul:0\", shape=(None, 2524, 12), dtype=float32)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2524)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 2524, 16)          776688    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 2524, 12)          1392      \n",
      "_________________________________________________________________\n",
      "attention_mech (AttentionMec ((None, 12), (None, 2524, 2536      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 780,629\n",
      "Trainable params: 780,629\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Attention, GlobalAveragePooling1D\n",
    "\n",
    "# Architecture\n",
    "inputs1 = Input(shape=(max_length,))\n",
    "embedding1 = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=max_length, embeddings_regularizer=l2(.001))\n",
    "embd_out1 = embedding1(inputs1)\n",
    "lstm1 = LSTM(12, dropout=0.3, recurrent_dropout=0.2, return_sequences=True)\n",
    "lstm_out1 = lstm1(embd_out1)\n",
    "\n",
    "# attention = GlobalAveragePooling1D(Attention()([lstm_out1, lstm_out1]))\n",
    "weigthed_out, weights = AttentionMech()(lstm_out1)\n",
    "\n",
    "prob1 = Dense(1, activation='sigmoid')\n",
    "outputs1 = prob1(weigthed_out)\n",
    "\n",
    "model1 = Model(inputs1, outputs1) # classifier\n",
    "attention_model = Model(inputs1, weights) # attention weights\n",
    "\n",
    "\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model_1/attention_mech_1/mul:0\", shape=(None, 2524, 12), dtype=float32)\n",
      "scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model_1/attention_mech_1/mul:0\", shape=(None, 2524, 12), dtype=float32)\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.8675 - acc: 0.5079scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model_1/attention_mech_1/mul:0\", shape=(None, 2524, 12), dtype=float32)\n",
      "164/164 [==============================] - 435s 3s/step - loss: 0.8668 - acc: 0.5078 - val_loss: 0.6928 - val_acc: 0.4936\n",
      "Epoch 2/10\n",
      "164/164 [==============================] - 427s 3s/step - loss: 0.6908 - acc: 0.5099 - val_loss: 0.6031 - val_acc: 0.8215\n",
      "Epoch 3/10\n",
      "164/164 [==============================] - 416s 3s/step - loss: 0.5563 - acc: 0.7906 - val_loss: 0.6999 - val_acc: 0.5775\n",
      "Epoch 4/10\n",
      "164/164 [==============================] - 448s 3s/step - loss: 0.6811 - acc: 0.5692 - val_loss: 0.6637 - val_acc: 0.5652\n",
      "Epoch 5/10\n",
      "164/164 [==============================] - 417s 3s/step - loss: 0.6574 - acc: 0.6149 - val_loss: 0.5380 - val_acc: 0.7765\n",
      "Epoch 6/10\n",
      "164/164 [==============================] - 410s 2s/step - loss: 0.5220 - acc: 0.7921 - val_loss: 0.5511 - val_acc: 0.7737\n",
      "Epoch 7/10\n",
      "164/164 [==============================] - 412s 3s/step - loss: 0.4871 - acc: 0.8164 - val_loss: 0.5443 - val_acc: 0.7803\n",
      "Epoch 8/10\n",
      "164/164 [==============================] - 421s 3s/step - loss: 0.4469 - acc: 0.8446 - val_loss: 0.5270 - val_acc: 0.7960\n",
      "Epoch 9/10\n",
      "164/164 [==============================] - 400s 2s/step - loss: 0.4485 - acc: 0.8452 - val_loss: 0.5115 - val_acc: 0.8036\n",
      "Epoch 10/10\n",
      "164/164 [==============================] - 457s 3s/step - loss: 0.4738 - acc: 0.8274 - val_loss: 0.4719 - val_acc: 0.8319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8ff3ee7a30>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model1.fit(x=X_train,y=y_train,\n",
    "          batch_size=80,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test,y_test)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"attention_mech_4/mul:0\", shape=(None, 2524, 24), dtype=float32)\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 2524)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 2524, 16)          776688    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 2524, 24)          2784      \n",
      "_________________________________________________________________\n",
      "attention_mech_4 (AttentionM ((None, 24), (None, 2524, 2548      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 782,045\n",
      "Trainable params: 782,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Attention, GlobalAveragePooling1D\n",
    "\n",
    "# Architecture\n",
    "inputs1 = Input(shape=(max_length,))\n",
    "embedding1 = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=max_length, embeddings_regularizer=l2(.001))\n",
    "embd_out1 = embedding1(inputs1)\n",
    "lstm1 = Bidirectional(LSTM(lstm_units, dropout=0.3, recurrent_dropout=0.2, return_sequences=True))\n",
    "lstm_out1 = lstm1(embd_out1)\n",
    "\n",
    "# attention = GlobalAveragePooling1D(Attention()([lstm_out1, lstm_out1]))\n",
    "weigthed_out, weights = AttentionMech()(lstm_out1)\n",
    "\n",
    "prob1 = Dense(1, activation='sigmoid')\n",
    "outputs1 = prob1(weigthed_out)\n",
    "\n",
    "model1 = Model(inputs1, outputs1) # classifier\n",
    "attention_model = Model(inputs1, weights) # attention weights\n",
    "\n",
    "\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model/attention_mech/mul:0\", shape=(None, 2524, 12), dtype=float32)\n",
      "scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model/attention_mech/mul:0\", shape=(None, 2524, 12), dtype=float32)\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.7485 - true_positives_1: 2494.0854 - precision_1: 0.5055 - true_negatives_1: 846.5976 - false_negatives_1: 817.0000 - false_positives_1: 2441.8598 - acc: 0.5062scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model/attention_mech/mul:0\", shape=(None, 2524, 12), dtype=float32)\n",
      "164/164 [==============================] - 423s 3s/step - loss: 0.7482 - true_positives_1: 2509.8909 - precision_1: 0.5055 - true_negatives_1: 850.3939 - false_negatives_1: 820.7758 - false_positives_1: 2457.5455 - acc: 0.5062 - val_loss: 0.6925 - val_true_positives_1: 1999.0000 - val_precision_1: 0.6231 - val_true_negatives_1: 2033.0000 - val_false_negatives_1: 1185.0000 - val_false_positives_1: 1209.0000 - val_acc: 0.6275\n",
      "Epoch 2/10\n",
      "164/164 [==============================] - 407s 2s/step - loss: 0.6523 - true_positives_1: 1539.5818 - precision_1: 0.6678 - true_negatives_1: 2672.3576 - false_negatives_1: 1786.6667 - false_positives_1: 640.0000 - acc: 0.5958 - val_loss: 0.4735 - val_true_positives_1: 2670.0000 - val_precision_1: 0.8271 - val_true_negatives_1: 2684.0000 - val_false_negatives_1: 514.0000 - val_false_positives_1: 558.0000 - val_acc: 0.8332\n",
      "Epoch 3/10\n",
      "164/164 [==============================] - 407s 2s/step - loss: 0.5247 - true_positives_1: 2735.4970 - precision_1: 0.7746 - true_negatives_1: 2484.3515 - false_negatives_1: 602.3818 - false_positives_1: 816.3758 - acc: 0.7920 - val_loss: 0.4767 - val_true_positives_1: 2626.0000 - val_precision_1: 0.8211 - val_true_negatives_1: 2670.0000 - val_false_negatives_1: 558.0000 - val_false_positives_1: 572.0000 - val_acc: 0.8242\n",
      "Epoch 4/10\n",
      "164/164 [==============================] - 402s 2s/step - loss: 0.4931 - true_positives_1: 2475.8545 - precision_1: 0.8471 - true_negatives_1: 2880.3576 - false_negatives_1: 843.4424 - false_positives_1: 438.9515 - acc: 0.8109 - val_loss: 0.5023 - val_true_positives_1: 2198.0000 - val_precision_1: 0.8736 - val_true_negatives_1: 2924.0000 - val_false_negatives_1: 986.0000 - val_false_positives_1: 318.0000 - val_acc: 0.7971\n",
      "Epoch 5/10\n",
      "164/164 [==============================] - 401s 2s/step - loss: 0.5089 - true_positives_1: 2131.0364 - precision_1: 0.8926 - true_negatives_1: 3037.6970 - false_negatives_1: 1225.3879 - false_positives_1: 244.4848 - acc: 0.7856 - val_loss: 0.5124 - val_true_positives_1: 1989.0000 - val_precision_1: 0.9016 - val_true_negatives_1: 3025.0000 - val_false_negatives_1: 1195.0000 - val_false_positives_1: 217.0000 - val_acc: 0.7803\n",
      "Epoch 6/10\n",
      "164/164 [==============================] - 404s 2s/step - loss: 0.4720 - true_positives_1: 2369.3394 - precision_1: 0.9031 - true_negatives_1: 3030.9697 - false_negatives_1: 957.4061 - false_positives_1: 280.8909 - acc: 0.8148 - val_loss: 0.4648 - val_true_positives_1: 2381.0000 - val_precision_1: 0.8851 - val_true_negatives_1: 2933.0000 - val_false_negatives_1: 803.0000 - val_false_positives_1: 309.0000 - val_acc: 0.8270\n",
      "Epoch 7/10\n",
      "164/164 [==============================] - 402s 2s/step - loss: 0.4447 - true_positives_1: 2643.1758 - precision_1: 0.8734 - true_negatives_1: 2931.4000 - false_negatives_1: 669.9939 - false_positives_1: 394.0364 - acc: 0.8443 - val_loss: 0.5140 - val_true_positives_1: 1973.0000 - val_precision_1: 0.9147 - val_true_negatives_1: 3058.0000 - val_false_negatives_1: 1211.0000 - val_false_positives_1: 184.0000 - val_acc: 0.7829\n",
      "Epoch 8/10\n",
      "164/164 [==============================] - 403s 2s/step - loss: 0.4454 - true_positives_1: 2716.4848 - precision_1: 0.8845 - true_negatives_1: 2919.6303 - false_negatives_1: 620.3818 - false_positives_1: 382.1091 - acc: 0.8426 - val_loss: 0.4642 - val_true_positives_1: 2941.0000 - val_precision_1: 0.7876 - val_true_negatives_1: 2449.0000 - val_false_negatives_1: 243.0000 - val_false_positives_1: 793.0000 - val_acc: 0.8388\n",
      "Epoch 9/10\n",
      "164/164 [==============================] - 388s 2s/step - loss: 0.4498 - true_positives_1: 3114.2364 - precision_1: 0.7907 - true_negatives_1: 2430.0364 - false_negatives_1: 235.1152 - false_positives_1: 859.2182 - acc: 0.8402 - val_loss: 0.4507 - val_true_positives_1: 2770.0000 - val_precision_1: 0.8288 - val_true_negatives_1: 2670.0000 - val_false_negatives_1: 414.0000 - val_false_positives_1: 572.0000 - val_acc: 0.8466\n",
      "Epoch 10/10\n",
      "164/164 [==============================] - 391s 2s/step - loss: 0.4137 - true_positives_1: 2969.5758 - precision_1: 0.8516 - true_negatives_1: 2784.8970 - false_negatives_1: 364.3273 - false_positives_1: 519.8061 - acc: 0.8663 - val_loss: 0.4384 - val_true_positives_1: 2824.0000 - val_precision_1: 0.8303 - val_true_negatives_1: 2665.0000 - val_false_negatives_1: 360.0000 - val_false_positives_1: 577.0000 - val_acc: 0.8542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feee2c955b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.keras.metrics\n",
    "\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.TruePositives(), \n",
    "                                                                      tf.keras.metrics.Precision(),\n",
    "                                                                      tf.keras.metrics.TrueNegatives(),\n",
    "                                                                      tf.keras.metrics.FalseNegatives(),\n",
    "                                                                      tf.keras.metrics.FalsePositives(), 'acc'])\n",
    "model1.fit(x=X_train,y=y_train,\n",
    "          batch_size=80,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test,y_test)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model/attention_mech/mul:0\", shape=(None, 2524, 12), dtype=float32)\n",
      "scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model/attention_mech/mul:0\", shape=(None, 2524, 12), dtype=float32)\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3778 - true_positives_2: 2987.1402 - precision_2: 0.8536 - true_negatives_2: 2793.6220 - false_negatives_2: 295.4085 - false_positives_2: 523.3720 - acc: 0.8777scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model/attention_mech/mul:0\", shape=(None, 2524, 12), dtype=float32)\n",
      "164/164 [==============================] - 410s 2s/step - loss: 0.3779 - true_positives_2: 3005.1818 - precision_2: 0.8536 - true_negatives_2: 2809.3212 - false_negatives_2: 297.1212 - false_positives_2: 526.9818 - acc: 0.8776 - val_loss: 0.4925 - val_true_positives_2: 3132.0000 - val_precision_2: 0.7156 - val_true_negatives_2: 1997.0000 - val_false_negatives_2: 52.0000 - val_false_positives_2: 1245.0000 - val_acc: 0.7982\n",
      "Epoch 2/15\n",
      "164/164 [==============================] - 404s 2s/step - loss: 0.3888 - true_positives_2: 2905.5879 - precision_2: 0.8725 - true_negatives_2: 2909.5455 - false_negatives_2: 431.5455 - false_positives_2: 391.9273 - acc: 0.8750 - val_loss: 0.4975 - val_true_positives_2: 2329.0000 - val_precision_2: 0.8726 - val_true_negatives_2: 2902.0000 - val_false_negatives_2: 855.0000 - val_false_positives_2: 340.0000 - val_acc: 0.8140\n",
      "Epoch 3/15\n",
      "164/164 [==============================] - 403s 2s/step - loss: 0.3822 - true_positives_2: 2876.5636 - precision_2: 0.9164 - true_negatives_2: 2997.1758 - false_negatives_2: 473.7152 - false_positives_2: 291.1515 - acc: 0.8832 - val_loss: 0.4476 - val_true_positives_2: 2473.0000 - val_precision_2: 0.9125 - val_true_negatives_2: 3005.0000 - val_false_negatives_2: 711.0000 - val_false_positives_2: 237.0000 - val_acc: 0.8525\n",
      "Epoch 4/15\n",
      "164/164 [==============================] - 403s 2s/step - loss: 0.4005 - true_positives_2: 3122.5697 - precision_2: 0.8227 - true_negatives_2: 2549.1515 - false_negatives_2: 197.8909 - false_positives_2: 768.9939 - acc: 0.8646 - val_loss: 0.3981 - val_true_positives_2: 2661.0000 - val_precision_2: 0.9170 - val_true_negatives_2: 3001.0000 - val_false_negatives_2: 523.0000 - val_false_positives_2: 241.0000 - val_acc: 0.8811\n",
      "Epoch 5/15\n",
      "164/164 [==============================] - 408s 2s/step - loss: 0.3194 - true_positives_2: 3067.2303 - precision_2: 0.9099 - true_negatives_2: 2992.1212 - false_negatives_2: 252.4303 - false_positives_2: 326.8242 - acc: 0.9144 - val_loss: 0.3841 - val_true_positives_2: 2787.0000 - val_precision_2: 0.8953 - val_true_negatives_2: 2916.0000 - val_false_negatives_2: 397.0000 - val_false_positives_2: 326.0000 - val_acc: 0.8875\n",
      "Epoch 6/15\n",
      "164/164 [==============================] - 399s 2s/step - loss: 0.4057 - true_positives_2: 3162.8424 - precision_2: 0.8081 - true_negatives_2: 2154.4667 - false_negatives_2: 155.4909 - false_positives_2: 1165.8061 - acc: 0.8462 - val_loss: 0.7296 - val_true_positives_2: 3184.0000 - val_precision_2: 0.4964 - val_true_negatives_2: 12.0000 - val_false_negatives_2: 0.0000e+00 - val_false_positives_2: 3230.0000 - val_acc: 0.4974\n",
      "Epoch 7/15\n",
      "164/164 [==============================] - 391s 2s/step - loss: 0.6457 - true_positives_2: 3221.7758 - precision_2: 0.5591 - true_negatives_2: 1038.9879 - false_negatives_2: 94.8667 - false_positives_2: 2282.9758 - acc: 0.5966 - val_loss: 0.4360 - val_true_positives_2: 3003.0000 - val_precision_2: 0.7928 - val_true_negatives_2: 2457.0000 - val_false_negatives_2: 181.0000 - val_false_positives_2: 785.0000 - val_acc: 0.8497\n",
      "Epoch 8/15\n",
      "164/164 [==============================] - 389s 2s/step - loss: 0.4054 - true_positives_2: 3147.3879 - precision_2: 0.8146 - true_negatives_2: 2624.3636 - false_negatives_2: 165.3818 - false_positives_2: 701.4727 - acc: 0.8670 - val_loss: 0.4342 - val_true_positives_2: 2887.0000 - val_precision_2: 0.8325 - val_true_negatives_2: 2661.0000 - val_false_negatives_2: 297.0000 - val_false_positives_2: 581.0000 - val_acc: 0.8634\n",
      "Epoch 9/15\n",
      "164/164 [==============================] - 393s 2s/step - loss: 0.3859 - true_positives_2: 3106.6061 - precision_2: 0.8423 - true_negatives_2: 2692.5030 - false_negatives_2: 199.1273 - false_positives_2: 640.3697 - acc: 0.8810 - val_loss: 0.5058 - val_true_positives_2: 3133.0000 - val_precision_2: 0.6768 - val_true_negatives_2: 1746.0000 - val_false_negatives_2: 51.0000 - val_false_positives_2: 1496.0000 - val_acc: 0.7593\n",
      "Epoch 10/15\n",
      "164/164 [==============================] - 400s 2s/step - loss: 0.4195 - true_positives_2: 3218.3333 - precision_2: 0.7844 - true_negatives_2: 2455.6667 - false_negatives_2: 133.2970 - false_positives_2: 831.3091 - acc: 0.8478 - val_loss: 0.4405 - val_true_positives_2: 2924.0000 - val_precision_2: 0.8184 - val_true_negatives_2: 2593.0000 - val_false_negatives_2: 260.0000 - val_false_positives_2: 649.0000 - val_acc: 0.8585\n",
      "Epoch 11/15\n",
      "164/164 [==============================] - 394s 2s/step - loss: 0.3866 - true_positives_2: 3121.6970 - precision_2: 0.8429 - true_negatives_2: 2733.8909 - false_negatives_2: 203.9879 - false_positives_2: 579.0303 - acc: 0.8835 - val_loss: 0.4329 - val_true_positives_2: 2892.0000 - val_precision_2: 0.8303 - val_true_negatives_2: 2651.0000 - val_false_negatives_2: 292.0000 - val_false_positives_2: 591.0000 - val_acc: 0.8626\n",
      "Epoch 12/15\n",
      "164/164 [==============================] - 392s 2s/step - loss: 0.3645 - true_positives_2: 3128.7758 - precision_2: 0.8553 - true_negatives_2: 2785.4727 - false_negatives_2: 189.0788 - false_positives_2: 535.2788 - acc: 0.8933 - val_loss: 0.4356 - val_true_positives_2: 2964.0000 - val_precision_2: 0.8068 - val_true_negatives_2: 2532.0000 - val_false_negatives_2: 220.0000 - val_false_positives_2: 710.0000 - val_acc: 0.8553\n",
      "Epoch 13/15\n",
      "164/164 [==============================] - 430s 3s/step - loss: 0.3725 - true_positives_2: 3125.0061 - precision_2: 0.8587 - true_negatives_2: 2778.0303 - false_negatives_2: 219.1939 - false_positives_2: 516.3758 - acc: 0.8908 - val_loss: 0.4314 - val_true_positives_2: 2866.0000 - val_precision_2: 0.8370 - val_true_negatives_2: 2684.0000 - val_false_negatives_2: 318.0000 - val_false_positives_2: 558.0000 - val_acc: 0.8637\n",
      "Epoch 14/15\n",
      "164/164 [==============================] - 431s 3s/step - loss: 0.3726 - true_positives_2: 3177.8909 - precision_2: 0.8446 - true_negatives_2: 2704.2242 - false_negatives_2: 165.2424 - false_positives_2: 591.2485 - acc: 0.8864 - val_loss: 0.4420 - val_true_positives_2: 2815.0000 - val_precision_2: 0.8489 - val_true_negatives_2: 2741.0000 - val_false_negatives_2: 369.0000 - val_false_positives_2: 501.0000 - val_acc: 0.8646\n",
      "Epoch 15/15\n",
      "164/164 [==============================] - 388s 2s/step - loss: 0.3873 - true_positives_2: 3054.2424 - precision_2: 0.8693 - true_negatives_2: 2812.6364 - false_negatives_2: 284.7515 - false_positives_2: 486.9758 - acc: 0.8876 - val_loss: 0.4427 - val_true_positives_2: 2870.0000 - val_precision_2: 0.8254 - val_true_negatives_2: 2635.0000 - val_false_negatives_2: 314.0000 - val_false_positives_2: 607.0000 - val_acc: 0.8567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feee01c70d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.keras.metrics\n",
    "\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.TruePositives(), \n",
    "                                                                      tf.keras.metrics.Precision(),\n",
    "                                                                      tf.keras.metrics.TrueNegatives(),\n",
    "                                                                      tf.keras.metrics.FalseNegatives(),\n",
    "                                                                      tf.keras.metrics.FalsePositives(), 'acc'])\n",
    "model1.fit(x=X_train,y=y_train,\n",
    "          batch_size=80,\n",
    "          epochs=15,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test,y_test)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "]\u001a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
