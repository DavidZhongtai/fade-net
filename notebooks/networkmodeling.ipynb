{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Convolution1D\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#import statments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bodytext</th>\n",
       "      <th>label</th>\n",
       "      <th>words</th>\n",
       "      <th>stem_meaningful</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guardian survey found pervasive culture staff ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[a, guardian, survey, has, found, that, there,...</td>\n",
       "      <td>[guardian, survey, found, pervasive, culture, ...</td>\n",
       "      <td>guardian survey found pervasive culture staff ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>store banning police launched investigation ga...</td>\n",
       "      <td>1</td>\n",
       "      <td>[store, how, about, banning, all, police, have...</td>\n",
       "      <td>[store, banning, police, launched, investigati...</td>\n",
       "      <td>store banning police launched investigation ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>defended predecessor trade leading delegation ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[has, defended, his, predecessor, as, trade, f...</td>\n",
       "      <td>[defended, predecessor, trade, leading, delega...</td>\n",
       "      <td>defended predecessor trade leading delegation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>email print jay z performed free get vote conc...</td>\n",
       "      <td>1</td>\n",
       "      <td>[email, print, and, jay, z, performed, for, fr...</td>\n",
       "      <td>[email, print, jay, z, performed, free, get, v...</td>\n",
       "      <td>email print jay z performed free get vote conc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>today given lifetime opportunity tell trump go...</td>\n",
       "      <td>1</td>\n",
       "      <td>[by, are, today, being, given, a, once, in, a,...</td>\n",
       "      <td>[today, given, lifetime, opportunity, tell, tr...</td>\n",
       "      <td>today given lifetime opportunity tell trump go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            bodytext label  \\\n",
       "0  guardian survey found pervasive culture staff ...     0   \n",
       "1  store banning police launched investigation ga...     1   \n",
       "2  defended predecessor trade leading delegation ...     0   \n",
       "3  email print jay z performed free get vote conc...     1   \n",
       "4  today given lifetime opportunity tell trump go...     1   \n",
       "\n",
       "                                               words  \\\n",
       "0  [a, guardian, survey, has, found, that, there,...   \n",
       "1  [store, how, about, banning, all, police, have...   \n",
       "2  [has, defended, his, predecessor, as, trade, f...   \n",
       "3  [email, print, and, jay, z, performed, for, fr...   \n",
       "4  [by, are, today, being, given, a, once, in, a,...   \n",
       "\n",
       "                                     stem_meaningful  \\\n",
       "0  [guardian, survey, found, pervasive, culture, ...   \n",
       "1  [store, banning, police, launched, investigati...   \n",
       "2  [defended, predecessor, trade, leading, delega...   \n",
       "3  [email, print, jay, z, performed, free, get, v...   \n",
       "4  [today, given, lifetime, opportunity, tell, tr...   \n",
       "\n",
       "                                           processed  \n",
       "0  guardian survey found pervasive culture staff ...  \n",
       "1  store banning police launched investigation ga...  \n",
       "2  defended predecessor trade leading delegation ...  \n",
       "3  email print jay z performed free get vote conc...  \n",
       "4  today given lifetime opportunity tell trump go...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/davidan/Downloads/fade-net/data/data_shuffled.csv')\n",
    "data['bodytext'] = data['bodytext'].str.lower()\n",
    "\n",
    "def identify_tokens(row):\n",
    "    news = row['bodytext']\n",
    "    tokens = nltk.word_tokenize(news)\n",
    "    # taken only words (not punctuation)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data['words'] = data.apply(identify_tokens, axis=1)\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['words']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data['stem_meaningful'] = data.apply(remove_stops, axis=1)\n",
    "\n",
    "def rejoin_words(row):\n",
    "    my_list = row['stem_meaningful']\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    return joined_words\n",
    "\n",
    "data['processed'] = data.apply(rejoin_words, axis=1)\n",
    "\n",
    "# cols_to_drop = ['words', 'stem_meaningful']\n",
    "# data.drop(cols_to_drop, inplace=True)\n",
    "\n",
    "data['bodytext'] = data['processed']\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcol = ['words', 'stem_meaningful', 'processed']\n",
    "data.drop(labels = dropcol, axis = 1)\n",
    "\n",
    "data.to_csv('/Users/davidan/Downloads/fade-net/data/tokenizedData.csv', index = False)\n",
    "\n",
    "#manually check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/davidan/Downloads/fade-net/data/tokenizedData.csv')\n",
    "data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 1. 1.]\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/davidan/Downloads/fade-net/data/tokenizedData.csv')\n",
    "\n",
    "\n",
    "news = data['bodytext'].values\n",
    "label = data['label'].values\n",
    "\n",
    "#delete the header file \n",
    "news = np.delete(news, (4396), axis=0)\n",
    "label = np.delete(label, (4396), axis=0)\n",
    "\n",
    "counter_news = 0\n",
    "\n",
    "for entry in news:\n",
    "    news[counter_news] = news[counter_news][:244]\n",
    "\n",
    "label = np.array(label)\n",
    "\n",
    "label = label.astype(np.float)\n",
    "\n",
    "\n",
    "\n",
    "print(label)\n",
    "print(type(label[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guardian survey found pervasive culture staff bullying parts detrimental impact staff wellbeing patient stories professionals submitted survey guardian diagnosed acute stress incident critically high blood frightened one man thought nothing sex\n"
     ]
    }
   ],
   "source": [
    "print(news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News: guardian survey found pervasive culture staff bullying parts detrimental impact staff wellbeing patient stories professionals submitted survey guardian diagnosed acute stress incident critically high blood frightened one man thought nothing sex\n",
      "Representation: \n",
      "[413, 2105, 92, 8134, 648, 444, 5276, 991, 9492, 532, 444, 5702, 2753, 600, 3014, 4103, 2105, 413, 4981, 5787, 2292, 1315, 7115, 146, 1053, 6897, 2, 97, 228, 188, 911]\n",
      "Average Length 244\n",
      "vocabulary size: 48543\n",
      "Max Length: 2524\n",
      "Min Length: 1\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(news)\n",
    "\n",
    "text_matrix = t.texts_to_sequences(news)\n",
    "\n",
    "print('News: ' + news[0])\n",
    "print('Representation: ')\n",
    "print(text_matrix[0])\n",
    "\n",
    "#find max length of vectors\n",
    "\n",
    "max_length = 0\n",
    "min_length = 1\n",
    "total_length = 0\n",
    "\n",
    "emb_dim = 16\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "for i in range(len(text_matrix)):\n",
    "    sent_length = len(text_matrix[i])\n",
    "    if max_length < sent_length:\n",
    "        max_length = sent_length\n",
    "        \n",
    "for i in range(len(text_matrix)):\n",
    "    sent_length = len(text_matrix[i])\n",
    "    if min_length > sent_length:\n",
    "        min_length = sent_length\n",
    "\n",
    "for i in range(len(text_matrix)):\n",
    "    sent_length = len(text_matrix[i])\n",
    "    total_length += sent_length\n",
    "\n",
    "print('Average Length %d' % int(total_length/19255))\n",
    "print('vocabulary size: %d'%vocab_size)\n",
    "print('Max Length: %d' % max_length)\n",
    "print('Min Length: %d' % min_length)\n",
    "\n",
    "print(type(text_matrix[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[413, 2105, 92, 8134, 648, 444]\n"
     ]
    }
   ],
   "source": [
    "print(text_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 64, 16)            776688    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 12)                1392      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 778,093\n",
      "Trainable params: 778,093\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "\n",
    "#split datasets \n",
    "#No need to post pad data since all have lengths \n",
    "text_matrix = pad_sequences(text_matrix, maxlen=max_length, padding='post')\n",
    "\n",
    "# text_matrix = np.array(text_matrix, dtype=None)\n",
    "# label = np.array(label, dtype= None)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_matrix, label, test_size = 0.33, random_state = 53)\n",
    "\n",
    "X_train = tf.ragged.constant(X_train)\n",
    "X_test = tf.ragged.constant(X_test)\n",
    "y_train = tf.ragged.constant(y_train)\n",
    "y_test = tf.ragged.constant(y_test)\n",
    "\n",
    "lstm_units = 12\n",
    "inputs = Input(shape = (64,))\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=244, embeddings_regularizer=l2(.001))\n",
    "embd_out = embedding(inputs)\n",
    "\n",
    "lstm = LSTM(lstm_units, dropout = 0.25, recurrent_dropout = 0.2)\n",
    "lstm_out = lstm(embd_out)\n",
    "\n",
    "prob = Dense(1, activation = 'relu')\n",
    "outputs = prob(lstm_out)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "164/164 [==============================] - 508s 3s/step - loss: 7.6728 - acc: 0.5058 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 2/10\n",
      "164/164 [==============================] - 440s 3s/step - loss: 7.7659 - acc: 0.4965 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 3/10\n",
      "164/164 [==============================] - 402s 2s/step - loss: 7.6853 - acc: 0.5018 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 4/10\n",
      "164/164 [==============================] - 405s 2s/step - loss: 7.7203 - acc: 0.4995 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 5/10\n",
      "164/164 [==============================] - 415s 3s/step - loss: 7.7882 - acc: 0.4951 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 6/10\n",
      "164/164 [==============================] - 402s 2s/step - loss: 7.6755 - acc: 0.5024 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 7/10\n",
      "164/164 [==============================] - 375s 2s/step - loss: 7.7031 - acc: 0.5006 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 8/10\n",
      "164/164 [==============================] - 370s 2s/step - loss: 7.7215 - acc: 0.4994 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 9/10\n",
      "164/164 [==============================] - 397s 2s/step - loss: 7.6644 - acc: 0.5031 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 10/10\n",
      "164/164 [==============================] - 394s 2s/step - loss: 7.6851 - acc: 0.5018 - val_loss: 7.6429 - val_acc: 0.5045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9e86fc7ee0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "model.fit(x=X_train,y=y_train,\n",
    "          batch_size=80,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test,y_test)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-b7f2503dd27b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_3' is not defined"
     ]
    }
   ],
   "source": [
    "model_3.save('model_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
