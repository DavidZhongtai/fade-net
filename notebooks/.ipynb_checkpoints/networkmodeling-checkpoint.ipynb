{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Convolution1D\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#import statments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bodytext</th>\n",
       "      <th>label</th>\n",
       "      <th>words</th>\n",
       "      <th>stem_meaningful</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guardian survey found pervasive culture staff ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[a, guardian, survey, has, found, that, there,...</td>\n",
       "      <td>[guardian, survey, found, pervasive, culture, ...</td>\n",
       "      <td>guardian survey found pervasive culture staff ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>store banning police launched investigation ga...</td>\n",
       "      <td>1</td>\n",
       "      <td>[store, how, about, banning, all, police, have...</td>\n",
       "      <td>[store, banning, police, launched, investigati...</td>\n",
       "      <td>store banning police launched investigation ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>defended predecessor trade leading delegation ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[has, defended, his, predecessor, as, trade, f...</td>\n",
       "      <td>[defended, predecessor, trade, leading, delega...</td>\n",
       "      <td>defended predecessor trade leading delegation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>email print jay z performed free get vote conc...</td>\n",
       "      <td>1</td>\n",
       "      <td>[email, print, and, jay, z, performed, for, fr...</td>\n",
       "      <td>[email, print, jay, z, performed, free, get, v...</td>\n",
       "      <td>email print jay z performed free get vote conc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>today given lifetime opportunity tell trump go...</td>\n",
       "      <td>1</td>\n",
       "      <td>[by, are, today, being, given, a, once, in, a,...</td>\n",
       "      <td>[today, given, lifetime, opportunity, tell, tr...</td>\n",
       "      <td>today given lifetime opportunity tell trump go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            bodytext label  \\\n",
       "0  guardian survey found pervasive culture staff ...     0   \n",
       "1  store banning police launched investigation ga...     1   \n",
       "2  defended predecessor trade leading delegation ...     0   \n",
       "3  email print jay z performed free get vote conc...     1   \n",
       "4  today given lifetime opportunity tell trump go...     1   \n",
       "\n",
       "                                               words  \\\n",
       "0  [a, guardian, survey, has, found, that, there,...   \n",
       "1  [store, how, about, banning, all, police, have...   \n",
       "2  [has, defended, his, predecessor, as, trade, f...   \n",
       "3  [email, print, and, jay, z, performed, for, fr...   \n",
       "4  [by, are, today, being, given, a, once, in, a,...   \n",
       "\n",
       "                                     stem_meaningful  \\\n",
       "0  [guardian, survey, found, pervasive, culture, ...   \n",
       "1  [store, banning, police, launched, investigati...   \n",
       "2  [defended, predecessor, trade, leading, delega...   \n",
       "3  [email, print, jay, z, performed, free, get, v...   \n",
       "4  [today, given, lifetime, opportunity, tell, tr...   \n",
       "\n",
       "                                           processed  \n",
       "0  guardian survey found pervasive culture staff ...  \n",
       "1  store banning police launched investigation ga...  \n",
       "2  defended predecessor trade leading delegation ...  \n",
       "3  email print jay z performed free get vote conc...  \n",
       "4  today given lifetime opportunity tell trump go...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/davidan/Downloads/fade-net/data/data_shuffled.csv')\n",
    "data['bodytext'] = data['bodytext'].str.lower()\n",
    "\n",
    "def identify_tokens(row):\n",
    "    news = row['bodytext']\n",
    "    tokens = nltk.word_tokenize(news)\n",
    "    # taken only words (not punctuation)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data['words'] = data.apply(identify_tokens, axis=1)\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['words']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data['stem_meaningful'] = data.apply(remove_stops, axis=1)\n",
    "\n",
    "def rejoin_words(row):\n",
    "    my_list = row['stem_meaningful']\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    return joined_words\n",
    "\n",
    "data['processed'] = data.apply(rejoin_words, axis=1)\n",
    "\n",
    "# cols_to_drop = ['words', 'stem_meaningful']\n",
    "# data.drop(cols_to_drop, inplace=True)\n",
    "\n",
    "data['bodytext'] = data['processed']\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcol = ['words', 'stem_meaningful', 'processed']\n",
    "data.drop(labels = dropcol, axis = 1)\n",
    "\n",
    "data.to_csv('/Users/davidan/Downloads/fade-net/data/tokenizedData.csv', index = False)\n",
    "\n",
    "#manually check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/davidan/Downloads/fade-net/data/tokenizedData.csv')\n",
    "data = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 1. 1.]\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/davidan/Downloads/fade-net/data/tokenizedData.csv')\n",
    "\n",
    "\n",
    "news = data['bodytext'].values\n",
    "label = data['label'].values\n",
    "\n",
    "#delete the header file \n",
    "news = np.delete(news, (4396), axis=0)\n",
    "label = np.delete(label, (4396), axis=0)\n",
    "\n",
    "counter_news = 0\n",
    "\n",
    "for entry in news:\n",
    "    news[counter_news] = news[counter_news][:244]\n",
    "\n",
    "label = np.array(label)\n",
    "\n",
    "label = label.astype(np.float)\n",
    "\n",
    "\n",
    "\n",
    "print(label)\n",
    "print(type(label[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guardian survey found pervasive culture staff bullying parts detrimental impact staff wellbeing patient stories professionals submitted survey guardian diagnosed acute stress incident critically high blood frightened one man thought nothing sex\n"
     ]
    }
   ],
   "source": [
    "print(news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News: guardian survey found pervasive culture staff bullying parts detrimental impact staff wellbeing patient stories professionals submitted survey guardian diagnosed acute stress incident critically high blood frightened one man thought nothing sex\n",
      "Representation: \n",
      "[413, 2105, 92, 8134, 648, 444, 5276, 991, 9492, 532, 444, 5702, 2753, 600, 3014, 4103, 2105, 413, 4981, 5787, 2292, 1315, 7115, 146, 1053, 6897, 2, 97, 228, 188, 911]\n",
      "Average Length 244\n",
      "vocabulary size: 48543\n",
      "Max Length: 2524\n",
      "Min Length: 1\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(news)\n",
    "\n",
    "text_matrix = t.texts_to_sequences(news)\n",
    "\n",
    "print('News: ' + news[0])\n",
    "print('Representation: ')\n",
    "print(text_matrix[0])\n",
    "\n",
    "#find max length of vectors\n",
    "\n",
    "max_length = 0\n",
    "min_length = 1\n",
    "total_length = 0\n",
    "\n",
    "emb_dim = 16\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "for i in range(len(text_matrix)):\n",
    "    sent_length = len(text_matrix[i])\n",
    "    if max_length < sent_length:\n",
    "        max_length = sent_length\n",
    "        \n",
    "for i in range(len(text_matrix)):\n",
    "    sent_length = len(text_matrix[i])\n",
    "    if min_length > sent_length:\n",
    "        min_length = sent_length\n",
    "\n",
    "for i in range(len(text_matrix)):\n",
    "    sent_length = len(text_matrix[i])\n",
    "    total_length += sent_length\n",
    "\n",
    "print('Average Length %d' % int(total_length/19255))\n",
    "print('vocabulary size: %d'%vocab_size)\n",
    "print('Max Length: %d' % max_length)\n",
    "print('Min Length: %d' % min_length)\n",
    "\n",
    "print(type(text_matrix[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[413, 2105, 92, 8134, 648, 444, 5276, 991, 9492, 532, 444, 5702, 2753, 600, 3014, 4103, 2105, 413, 4981, 5787, 2292, 1315, 7115, 146, 1053, 6897, 2, 97, 228, 188, 911]\n"
     ]
    }
   ],
   "source": [
    "print(text_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split datasets \n",
    "#No need to post pad data since all have lengths \n",
    "text_matrix = pad_sequences(text_matrix, maxlen=max_length, padding='post')\n",
    "\n",
    "# text_matrix = np.array(text_matrix, dtype=None)\n",
    "# label = np.array(label, dtype= None)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_matrix, label, test_size = 0.33, random_state = 53)\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "X_test = tf.convert_to_tensor(X_test)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_test = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 64, 16)            776688    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 12)                1392      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 778,093\n",
      "Trainable params: 778,093\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "lstm_units = 12\n",
    "inputs = Input(shape = (64,))\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=244, embeddings_regularizer=l2(.001))\n",
    "embd_out = embedding(inputs)\n",
    "\n",
    "lstm = LSTM(lstm_units, dropout = 0.25, recurrent_dropout = 0.2)\n",
    "lstm_out = lstm(embd_out)\n",
    "\n",
    "prob = Dense(1, activation = 'relu')\n",
    "outputs = prob(lstm_out)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "164/164 [==============================] - 508s 3s/step - loss: 7.6728 - acc: 0.5058 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 2/10\n",
      "164/164 [==============================] - 440s 3s/step - loss: 7.7659 - acc: 0.4965 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 3/10\n",
      "164/164 [==============================] - 402s 2s/step - loss: 7.6853 - acc: 0.5018 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 4/10\n",
      "164/164 [==============================] - 405s 2s/step - loss: 7.7203 - acc: 0.4995 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 5/10\n",
      "164/164 [==============================] - 415s 3s/step - loss: 7.7882 - acc: 0.4951 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 6/10\n",
      "164/164 [==============================] - 402s 2s/step - loss: 7.6755 - acc: 0.5024 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 7/10\n",
      "164/164 [==============================] - 375s 2s/step - loss: 7.7031 - acc: 0.5006 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 8/10\n",
      "164/164 [==============================] - 370s 2s/step - loss: 7.7215 - acc: 0.4994 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 9/10\n",
      "164/164 [==============================] - 397s 2s/step - loss: 7.6644 - acc: 0.5031 - val_loss: 7.6429 - val_acc: 0.5045\n",
      "Epoch 10/10\n",
      "164/164 [==============================] - 394s 2s/step - loss: 7.6851 - acc: 0.5018 - val_loss: 7.6429 - val_acc: 0.5045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9e86fc7ee0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "model.fit(x=X_train,y=y_train,\n",
    "          batch_size=80,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test,y_test)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMech(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionMech, self).__init__(**kwargs)\n",
    "    \n",
    "    # This method states the weights that the layer will learn. It has as input param the shape of the input\n",
    "    # which is called. This method is called at the declaration time\n",
    "    def build(self, input_shape):\n",
    "        # We need to provide the dimensions of our weights. In this example, we will have a W_a matrix of\n",
    "        # dimension (lstm_units, 1), and a bias of dimension (max_length, 1)\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(AttentionMech, self).build(input_shape)\n",
    "    \n",
    "    # In this method with do all the calculations of the layer and return the output of the layer\n",
    "    def call(self, x):\n",
    "        # x is the input of the layer. In this example, the output of lstm (hidden_statesxlstm_units) \n",
    "        # hidden_states = max_length\n",
    "        \n",
    "        # We calculate the score tanh(W.x + b)\n",
    "        scores = K.tanh(K.dot(x,self.W)+self.b)  # (max_length x 1) \n",
    "        print('scores shape: ')\n",
    "        print(scores.shape)\n",
    "        \n",
    "        # This removes the last axis -> a vector of max_length dimension \n",
    "        # we can omit this since our W matrix has dimension 1 in the last axis\n",
    "        scores=K.squeeze(scores, axis=-1) \n",
    "        print('scores shape after squeeze: ')\n",
    "        print(scores.shape)\n",
    "        \n",
    "        # we apply softmax (the last axis is the default axis used for calculation)\n",
    "        at=K.softmax(scores)\n",
    "        print('attention weights shape: ')\n",
    "        print(at.shape)\n",
    "        \n",
    "        # This adds a 1-sized dimension to the last axis -> matrix of (max_length x 1)\n",
    "        at=K.expand_dims(at,axis=-1) # if there is no squeeze, then we can omit this\n",
    "        print('attention weights shape after expand_dims: ')\n",
    "        print(at.shape)\n",
    "        \n",
    "        # We calculate the weighted values -> \\alpha*hidden_states         \n",
    "        # row-wise multiplication (we are weighting the hidden_states, not the lstm_units) \n",
    "        output=x*at # (max_length x lstm_units)\n",
    "        print('weighted values shape: ')\n",
    "        print(output)\n",
    "        \n",
    "        # The output of this layer is the weighted values (we sum up the values of the hidden states), and\n",
    "        # the weights of the attetnion (max_length x 1)\n",
    "        return K.sum(output, axis=1), at\n",
    "    \n",
    "    # This is used for summary, to see the output shape of the two output matrices\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "    \n",
    "    # This is used for summary (it returns the params of the layer)\n",
    "    def get_config(self):\n",
    "        return super(BahdanauAttention, self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"attention_mech_1/mul:0\", shape=(None, 2524, 12), dtype=float32)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 2524)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 2524, 16)          776688    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 2524, 12)          1392      \n",
      "_________________________________________________________________\n",
      "attention_mech_1 (AttentionM ((None, 12), (None, 2524, 2536      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 780,629\n",
      "Trainable params: 780,629\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Attention, GlobalAveragePooling1D\n",
    "\n",
    "# Architecture\n",
    "inputs1 = Input(shape=(max_length,))\n",
    "embedding1 = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=max_length, embeddings_regularizer=l2(.001))\n",
    "embd_out1 = embedding1(inputs1)\n",
    "lstm1 = LSTM(lstm_units, dropout=0.3, recurrent_dropout=0.2, return_sequences=True)\n",
    "lstm_out1 = lstm1(embd_out1)\n",
    "\n",
    "# attention = GlobalAveragePooling1D(Attention()([lstm_out1, lstm_out1]))\n",
    "weigthed_out, weights = AttentionMech()(lstm_out1)\n",
    "\n",
    "prob1 = Dense(1, activation='sigmoid')\n",
    "outputs1 = prob1(weigthed_out)\n",
    "\n",
    "model1 = Model(inputs1, outputs1) # classifier\n",
    "attention_model = Model(inputs1, weights) # attention weights\n",
    "\n",
    "\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model_1/attention_mech_1/mul:0\", shape=(None, 2524, 12), dtype=float32)\n",
      "scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model_1/attention_mech_1/mul:0\", shape=(None, 2524, 12), dtype=float32)\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.8675 - acc: 0.5079scores shape: \n",
      "(None, 2524, 1)\n",
      "scores shape after squeeze: \n",
      "(None, 2524)\n",
      "attention weights shape: \n",
      "(None, 2524)\n",
      "attention weights shape after expand_dims: \n",
      "(None, 2524, 1)\n",
      "weighted values shape: \n",
      "Tensor(\"model_1/attention_mech_1/mul:0\", shape=(None, 2524, 12), dtype=float32)\n",
      "164/164 [==============================] - 435s 3s/step - loss: 0.8668 - acc: 0.5078 - val_loss: 0.6928 - val_acc: 0.4936\n",
      "Epoch 2/10\n",
      "164/164 [==============================] - 427s 3s/step - loss: 0.6908 - acc: 0.5099 - val_loss: 0.6031 - val_acc: 0.8215\n",
      "Epoch 3/10\n",
      "164/164 [==============================] - 416s 3s/step - loss: 0.5563 - acc: 0.7906 - val_loss: 0.6999 - val_acc: 0.5775\n",
      "Epoch 4/10\n",
      "164/164 [==============================] - 448s 3s/step - loss: 0.6811 - acc: 0.5692 - val_loss: 0.6637 - val_acc: 0.5652\n",
      "Epoch 5/10\n",
      "164/164 [==============================] - 417s 3s/step - loss: 0.6574 - acc: 0.6149 - val_loss: 0.5380 - val_acc: 0.7765\n",
      "Epoch 6/10\n",
      "164/164 [==============================] - 410s 2s/step - loss: 0.5220 - acc: 0.7921 - val_loss: 0.5511 - val_acc: 0.7737\n",
      "Epoch 7/10\n",
      "164/164 [==============================] - 412s 3s/step - loss: 0.4871 - acc: 0.8164 - val_loss: 0.5443 - val_acc: 0.7803\n",
      "Epoch 8/10\n",
      "164/164 [==============================] - 421s 3s/step - loss: 0.4469 - acc: 0.8446 - val_loss: 0.5270 - val_acc: 0.7960\n",
      "Epoch 9/10\n",
      "164/164 [==============================] - 400s 2s/step - loss: 0.4485 - acc: 0.8452 - val_loss: 0.5115 - val_acc: 0.8036\n",
      "Epoch 10/10\n",
      "164/164 [==============================] - 457s 3s/step - loss: 0.4738 - acc: 0.8274 - val_loss: 0.4719 - val_acc: 0.8319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8ff3ee7a30>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model1.fit(x=X_train,y=y_train,\n",
    "          batch_size=80,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test,y_test)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model1.fit(x=X_train,y=y_train,\n",
    "          batch_size=100,\n",
    "          epochs=15,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_test,y_test)\n",
    "          )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
